dataset: 
  train:
    target: taxa_diffusion.data.INaturalistDataset
    params:
      json_path: 'url/to/iNaturalist_2021/train.json'
      img_size: 512
      fallback_img_path: 'train/07646_Plantae_Tracheophyta_Magnoliopsida_Dipsacales_Caprifoliaceae_Lonicera_hispidula/b3979d80-cefb-43d3-bd61-260b925606ad.jpg'
      condition_prob: 0.05
      mapping_file: 'condition_mappings.txt'
      training: True
      base_image_url: 'url/to/iNaturalist_2021'
  validation:
      num_inference_steps: 25
      guidance_scale: 9.0
      target: taxa_diffusion.data.INaturalistDataset
      params:
        json_path: 'subset_dataset.json'
        img_size: 512
        fallback_img_path: 'val/07646_Plantae_Tracheophyta_Magnoliopsida_Dipsacales_Caprifoliaceae_Lonicera_hispidula/b3979d80-cefb-43d3-bd61-260b925606ad.jpg'
        condition_prob: 0.0
        mapping_file: 'condition_mappings.txt'
        training: False
        base_image_url: '/fs/scratch/PAS0536/amin/iNaturalist_2021'
  test:
    level_name: 'kingdom'
    sample_number: 500
    resolution: 512
    mapping_file: 'condition_mappings.txt'
    noise_step: 750
model:
  target: taxa_diffusion.models.TaxonomyModel
  params:
    word_dim: 768
    nhead: 8
    level_number: 7
    num_layers_per_level: 2
train:
  range_guidance: [0.0, 1.0]
  do_new_guidance: True
  level_number: 7
  add_lora_unet: True
  lora_rank: 4
  taxonomy_cut_off: 1
  pretrained_model_path: 'ddpm_models'
  pretrained_model_path_last_part: ''
  pretrained_lora_model: '/users/PAS0536/aminr8/TaxonomyGen/output/v3_taxonomy_gen_osc-2024-11-02T10-55-51_taxonomy_cut_off_0/checkpoints/lora_checkpoint-epoch-20/pytorch_lora_weights.safetensors'
  checkpoint_path: '/users/PAS0536/aminr8/TaxonomyGen/output/v3_taxonomy_gen_osc-2024-11-02T16-15-58/checkpoints/checkpoint-epoch-8.ckpt'
  is_debug: False
  global_seed: 42
  output_dir: 'output'
  num_workers: 16
  train_batch_size: 32
  valid_batch_size: 1
  noise_scheduler_kwargs: 
    num_train_timesteps: 1000
    beta_start:          0.00085
    beta_end:            0.012
    beta_schedule:       "linear"
    steps_offset:        1
    clip_sample:         false
  max_train_epoch: 20
  max_train_steps: -1
  scale_lr: False
  checkpointing_epochs: 1
  checkpointing_steps: -1
  validation_steps: 1000
  validation_steps_tuple: [1, 50, 100, 400, 750]
  gradient_checkpointing: False
  mixed_precision_training: False
  max_grad_norm: 1
  enable_xformers_memory_efficient_attention: False
optimize:
  learning_rate: 1e-4
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_weight_decay: 1e-2
  adam_epsilon: 1e-08
  lr_warmup_steps: 500
  lr_scheduler: "constant"
  gradient_accumulation_steps: 1
